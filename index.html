<!DOCTYPE html>
<html lang="vi">
<head>
<meta charset="UTF-8">
<title>LeKimLam AI - HUD VISION SYSTEM</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<!-- Font Cyberpunk -->
<link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<style>
    :root {
        --primary: #00f3ff; /* Cyan Cyberpunk */
        --warning: #ff0055; /* Red Alert */
        --bg-hud: rgba(10, 15, 20, 0.85);
    }
    
    body { 
        margin: 0; background: #000; overflow: hidden; 
        font-family: 'Share Tech Mono', monospace; 
    }

    /* Hi·ªáu ·ª©ng l∆∞·ªõi qu√©t (Grid Overlay) */
    #grid-overlay {
        position: fixed; top: 0; left: 0; width: 100vw; height: 100vh;
        background: 
            linear-gradient(rgba(0, 243, 255, 0.03) 1px, transparent 1px),
            linear-gradient(90deg, rgba(0, 243, 255, 0.03) 1px, transparent 1px);
        background-size: 50px 50px;
        pointer-events: none; z-index: 5;
    }

    /* Container ch√≠nh */
    #viewport { position: relative; width: 100vw; height: 100vh; overflow: hidden; }
    
    video { 
        position: absolute; width: 100%; height: 100%; 
        object-fit: cover; filter: contrast(1.1) saturate(1.2); /* TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n cho gi·ªëng phim */
    }
    
    canvas { position: absolute; width: 100%; height: 100%; object-fit: cover; pointer-events: none; z-index: 10; }

    /* UI Layer */
    #ui-layer { position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: 20; pointer-events: none; }

    /* Top Bar */
    .top-hud {
        position: absolute; top: 0; left: 0; width: 100%; padding: 15px;
        display: flex; justify-content: space-between;
        background: linear-gradient(to bottom, rgba(0,0,0,0.8), transparent);
        color: var(--primary); text-shadow: 0 0 5px var(--primary);
    }
    .system-status { font-size: 14px; display: flex; gap: 20px; }
    .blink { animation: blinker 1s linear infinite; }

    /* Controls */
    .controls {
        position: absolute; bottom: 30px; width: 100%;
        display: flex; justify-content: center; gap: 20px; pointer-events: auto;
    }

    button {
        background: rgba(0, 20, 30, 0.6); border: 1px solid var(--primary);
        color: var(--primary); padding: 10px 20px; font-family: 'Share Tech Mono';
        text-transform: uppercase; letter-spacing: 2px; cursor: pointer;
        backdrop-filter: blur(4px); box-shadow: 0 0 10px rgba(0, 243, 255, 0.2);
        transition: 0.3s; clip-path: polygon(10px 0, 100% 0, 100% calc(100% - 10px), calc(100% - 10px) 100%, 0 100%, 0 10px);
    }
    button:hover { background: var(--primary); color: #000; box-shadow: 0 0 20px var(--primary); }

    @keyframes blinker { 50% { opacity: 0; } }

    /* Loading Screen */
    #loader {
        position: fixed; top: 0; left: 0; width: 100%; height: 100%;
        background: #000; z-index: 100;
        display: flex; flex-direction: column; align-items: center; justify-content: center;
        color: var(--primary);
    }
    .loading-bar {
        width: 200px; height: 4px; background: #333; margin-top: 20px;
        position: relative; overflow: hidden;
    }
    .loading-bar::after {
        content: ''; position: absolute; top: 0; left: 0; height: 100%; width: 50%;
        background: var(--primary); animation: load 1s infinite ease-in-out;
    }
    @keyframes load { 0% { left: -50%; } 100% { left: 100%; } }

</style>
</head>
<body>

<div id="loader">
    <h1>SYSTEM INITIALIZING...</h1>
    <div class="loading-bar"></div>
    <p id="load-text">LOADING NEURAL NETWORK [MOBILENET_V2]...</p>
</div>

<div id="grid-overlay"></div>

<div id="viewport">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
</div>

<div id="ui-layer">
    <div class="top-hud">
        <div class="system-status">
            <span>SYS: <span style="color:#0f0">ONLINE</span></span>
            <span>CPU: <span id="fps-meter">0</span> FPS</span>
            <span>TARGETS: <span id="target-count">0</span></span>
        </div>
        <div class="blink">‚óè REC</div>
    </div>

    <div class="controls">
        <button onclick="switchCamera()">[CAM_SWITCH]</button>
        <button onclick="toggleMode()">[MODE: HUD]</button>
    </div>
</div>

<script>
/**
 * LEKIMLAM ARCHITECTURE - HUD CORE
 */

// --- 1. ADVANCED DICTIONARY (T·ª™ ƒêI·ªÇN SI√äU C·∫§P) ---
const CODENAMES = {
    'person':       { name: 'BIO-UNIT: HUMAN', type: 'BIO', color: '#00f3ff' },
    'bicycle':      { name: 'VEHICLE: BICYCLE', type: 'MECH', color: '#ffaa00' },
    'car':          { name: 'VEHICLE: AUTO-4', type: 'MECH', color: '#ffaa00' },
    'motorcycle':   { name: 'VEHICLE: MOTO-2', type: 'MECH', color: '#ffaa00' },
    'airplane':     { name: 'AERIAL: PLANE', type: 'MECH', color: '#ff0055' },
    'bus':          { name: 'TRANSIT: BUS', type: 'MECH', color: '#ffaa00' },
    'train':        { name: 'TRANSIT: RAIL', type: 'MECH', color: '#ffaa00' },
    'truck':        { name: 'HAULER: TRUCK', type: 'MECH', color: '#ffaa00' },
    'boat':         { name: 'MARINE: VESSEL', type: 'MECH', color: '#00aaff' },
    'traffic light':{ name: 'SIGNAL: TRAFFIC', type: 'INFRA', color: '#ffff00' },
    'cat':          { name: 'FAUNA: FELINE', type: 'BIO', color: '#ff00ff' },
    'dog':          { name: 'FAUNA: CANINE', type: 'BIO', color: '#ff00ff' },
    'horse':        { name: 'FAUNA: EQUINE', type: 'BIO', color: '#ff00ff' },
    'backpack':     { name: 'STORAGE: PACK', type: 'OBJ', color: '#00ff00' },
    'umbrella':     { name: 'SHIELD: RAIN', type: 'OBJ', color: '#00ff00' },
    'bottle':       { name: 'CONTAINER: FLUID', type: 'OBJ', color: '#00ff00' },
    'cup':          { name: 'VESSEL: DRINK', type: 'OBJ', color: '#00ff00' },
    'fork':         { name: 'TOOL: FEEDING', type: 'OBJ', color: '#cccccc' },
    'knife':        { name: 'WEAPON: BLADE', type: 'WARN', color: '#ff0000' },
    'spoon':        { name: 'TOOL: SCOOP', type: 'OBJ', color: '#cccccc' },
    'bowl':         { name: 'VESSEL: BOWL', type: 'OBJ', color: '#cccccc' },
    'banana':       { name: 'FLORA: FRUIT-Y', type: 'BIO', color: '#ffff00' },
    'apple':        { name: 'FLORA: FRUIT-R', type: 'BIO', color: '#ff0000' },
    'sandwich':     { name: 'RATION: BREAD', type: 'BIO', color: '#ffaa00' },
    'orange':       { name: 'FLORA: CITRUS', type: 'BIO', color: '#ffaa00' },
    'chair':        { name: 'STRUCT: SEAT', type: 'INFRA', color: '#00aaff' },
    'couch':        { name: 'STRUCT: SOFA', type: 'INFRA', color: '#00aaff' },
    'potted plant': { name: 'FLORA: DECOR', type: 'BIO', color: '#00ff00' },
    'bed':          { name: 'STRUCT: SLEEP', type: 'INFRA', color: '#00aaff' },
    'tv':           { name: 'DEVICE: DISPLAY', type: 'TECH', color: '#00f3ff' },
    'laptop':       { name: 'DEVICE: PORTABLE-PC', type: 'TECH', color: '#00f3ff' },
    'mouse':        { name: 'INPUT: MOUSE', type: 'TECH', color: '#00f3ff' },
    'remote':       { name: 'CTRL: REMOTE', type: 'TECH', color: '#00f3ff' },
    'keyboard':     { name: 'INPUT: KEYBOARD', type: 'TECH', color: '#00f3ff' },
    'cell phone':   { name: 'DEVICE: SMART-COM', type: 'TECH', color: '#00f3ff' },
    'book':         { name: 'DATA: PHYSICAL', type: 'OBJ', color: '#ffffff' },
    'clock':        { name: 'CHRONO: METER', type: 'TECH', color: '#ffffff' },
    'vase':         { name: 'DECOR: CERAMIC', type: 'OBJ', color: '#ffffff' },
    'scissors':     { name: 'TOOL: CUTTER', type: 'WARN', color: '#ff0055' },
    'teddy bear':   { name: 'TOY: PLUSH', type: 'OBJ', color: '#ff00ff' },
    'toothbrush':   { name: 'HYGIENE: BRUSH', type: 'OBJ', color: '#ffffff' }
};

// Fallback n·∫øu kh√¥ng t√¨m th·∫•y
function resolveName(originalClass) {
    if (CODENAMES[originalClass]) {
        return CODENAMES[originalClass];
    }
    return { 
        name: `UNKNOWN: ${originalClass.toUpperCase()}`, 
        type: 'UNK', 
        color: '#ffffff' 
    };
}

// --- 2. CORE CONFIG ---
const CONFIG = {
    SMOOTH: 0.25,
    IOU_THRESH: 0.35,
    CONFIDENCE: 0.55, // TƒÉng l√™n ƒë·ªÉ l·ªçc nhi·ªÖu
    MISS_LIMIT: 15
};

let state = {
    model: null,
    stream: null,
    facing: 'environment',
    tracks: [],
    lastTime: 0,
    fps: 0
};

const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');

// --- 3. LOGIC (TRACKING & IOU) ---
function getIoU(box1, box2) {
    const [x1, y1, w1, h1] = box1;
    const [x2, y2, w2, h2] = box2;
    const xi1 = Math.max(x1, x2);
    const yi1 = Math.max(y1, y2);
    const xi2 = Math.min(x1 + w1, x2 + w2);
    const yi2 = Math.min(y1 + h1, y2 + h2);
    const inter = Math.max(0, xi2 - xi1) * Math.max(0, yi2 - yi1);
    return inter / ((w1*h1) + (w2*h2) - inter);
}

function lerp(s, e, t) { return (1-t)*s + t*e; }

function updateTracks(preds) {
    state.tracks.forEach(t => t.seen = false);

    preds.forEach(p => {
        if (p.score < CONFIG.CONFIDENCE) return;

        let best = null, maxIoU = 0;
        state.tracks.forEach(t => {
            if (t.rawClass === p.class) {
                const iou = getIoU(t.box, p.bbox);
                if (iou > maxIoU) { maxIoU = iou; best = t; }
            }
        });

        if (best && maxIoU > CONFIG.IOU_THRESH) {
            best.target = p.bbox;
            best.score = p.score;
            best.seen = true;
            best.miss = 0;
        } else {
            const info = resolveName(p.class);
            state.tracks.push({
                id: Math.random().toString(16).slice(2, 6).toUpperCase(),
                rawClass: p.class,
                displayName: info.name,
                color: info.color,
                score: p.score,
                box: [...p.bbox],
                target: [...p.bbox],
                seen: true,
                miss: 0,
                // T·∫°o s·ªë li·ªáu gi·∫£ ng·∫´u nhi√™n cho ng·∫ßu
                dist: (Math.random() * 5 + 1).toFixed(1) + 'M',
                scanLine: 0 
            });
        }
    });

    state.tracks = state.tracks.filter(t => {
        if (!t.seen) t.miss++;
        return t.miss < CONFIG.MISS_LIMIT;
    });
    
    document.getElementById('target-count').innerText = state.tracks.length;
}

// --- 4. RENDERER (HUD STYLE) ---
function drawHUD(track) {
    // Smooth movement
    track.box[0] = lerp(track.box[0], track.target[0], CONFIG.SMOOTH);
    track.box[1] = lerp(track.box[1], track.target[1], CONFIG.SMOOTH);
    track.box[2] = lerp(track.box[2], track.target[2], CONFIG.SMOOTH);
    track.box[3] = lerp(track.box[3], track.target[3], CONFIG.SMOOTH);

    const [x, y, w, h] = track.box;
    const color = track.color;
    
    // Fade out if missing
    ctx.globalAlpha = Math.max(0.2, 1 - (track.miss / CONFIG.MISS_LIMIT));
    
    ctx.strokeStyle = color;
    ctx.lineWidth = 2;
    ctx.fillStyle = color;

    // 1. V·∫Ω 4 g√≥c (Corner Brackets) thay v√¨ full box
    const len = Math.min(w, h) * 0.2; // ƒê·ªô d√†i g√≥c
    
    // Top-Left
    ctx.beginPath(); ctx.moveTo(x, y + len); ctx.lineTo(x, y); ctx.lineTo(x + len, y); ctx.stroke();
    // Top-Right
    ctx.beginPath(); ctx.moveTo(x + w - len, y); ctx.lineTo(x + w, y); ctx.lineTo(x + w, y + len); ctx.stroke();
    // Bottom-Right
    ctx.beginPath(); ctx.moveTo(x + w, y + h - len); ctx.lineTo(x + w, y + h); ctx.lineTo(x + w - len, y + h); ctx.stroke();
    // Bottom-Left
    ctx.beginPath(); ctx.moveTo(x + len, y + h); ctx.lineTo(x, y + h); ctx.lineTo(x, y + h - len); ctx.stroke();

    // 2. Scan Line Effect (Ch·∫°y l√™n xu·ªëng trong box)
    track.scanLine += 2;
    if (track.scanLine > h) track.scanLine = 0;
    ctx.globalAlpha = 0.3 * ctx.globalAlpha;
    ctx.fillRect(x, y + track.scanLine, w, 2);
    ctx.globalAlpha = ctx.globalAlpha / 0.3; // Reset alpha

    // 3. Label & Data Block
    ctx.font = '12px "Share Tech Mono"';
    const label = `${track.displayName}`;
    const metrics = `ID:${track.id} | INTEG:${Math.round(track.score*100)}%`;
    
    // Background cho text
    ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
    ctx.fillRect(x, y - 40, Math.max(w, 150), 35);
    
    // Text ch√≠nh
    ctx.fillStyle = color;
    ctx.fillText(label, x + 5, y - 25);
    
    // Text ph·ª• (nh·ªè h∆°n)
    ctx.fillStyle = '#ccc';
    ctx.font = '10px "Share Tech Mono"';
    ctx.fillText(metrics, x + 5, y - 10);

    // 4. Connecting Line (ƒê∆∞·ªùng n·ªëi text v√†o box)
    ctx.beginPath();
    ctx.moveTo(x, y - 5);
    ctx.lineTo(x, y);
    ctx.stroke();

    // 5. T·ªça ƒë·ªô gi·∫£ l·∫≠p b√™n c·∫°nh box
    if (h > 100) {
        ctx.fillStyle = color;
        ctx.fillText(`Y:${Math.round(y)}`, x + w + 5, y + 20);
        ctx.fillText(`X:${Math.round(x)}`, x + w + 5, y + 35);
        ctx.fillText(`Z:${track.dist}`, x + w + 5, y + 50);
    }

    ctx.globalAlpha = 1.0;
}

function renderLoop() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    state.tracks.forEach(drawHUD);

    // T√≠nh FPS
    const now = performance.now();
    const delta = now - state.lastTime;
    if (delta >= 1000) {
        state.fps = Math.round((1000/delta) * 60); // ∆Ø·ªõc l∆∞·ª£ng
        document.getElementById('fps-meter').innerText = 60; // WebGL v·∫Ω 60fps
        state.lastTime = now;
    }

    requestAnimationFrame(renderLoop);
}

async function aiLoop() {
    if (!state.model) return;
    try {
        const preds = await state.model.detect(video);
        updateTracks(preds);
    } catch (e) {}
    requestAnimationFrame(aiLoop);
}

// --- 5. SETUP ---
async function main() {
    try {
        // Camera Setup
        const stream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: state.facing, width: { ideal: 1280 }, height: { ideal: 720 } },
            audio: false
        });
        video.srcObject = stream;
        
        // Load Model (D√πng mobilenet_v2 thay v√¨ lite ƒë·ªÉ ch√≠nh x√°c h∆°n)
        // L∆∞u √Ω: mobilenet_v2 n·∫∑ng h∆°n lite m·ªôt ch√∫t nh∆∞ng t·ªët h∆°n.
        state.model = await cocoSsd.load({ base: 'mobilenet_v2' });
        
        document.getElementById('loader').style.display = 'none';
        
        video.play();
        aiLoop();
        renderLoop();

    } catch (err) {
        document.getElementById('load-text').innerText = "ERROR: " + err.message;
        document.getElementById('load-text').style.color = 'red';
    }
}

function switchCamera() {
    state.facing = state.facing === 'user' ? 'environment' : 'user';
    main(); // Restart
}

function toggleMode() {
    // Ch·ª©c nƒÉng m·ªü r·ªông trong t∆∞∆°ng lai
    alert("MODE: HUD VISION [LOCKED]");
}

main();

</script>
</body>
</html>    }

    .controls-area {
        position: absolute; bottom: 30px; left: 0; width: 100%;
        display: flex; flex-direction: column; align-items: center; gap: 15px;
        pointer-events: auto;
    }

    .zoom-slider-container {
        display: flex; align-items: center; gap: 10px;
        background: var(--bg-glass); padding: 10px 20px; border-radius: 30px;
        backdrop-filter: blur(5px);
    }
    .zoom-val { color: white; min-width: 40px; text-align: center; font-variant-numeric: tabular-nums; }
    input[type=range] { width: 150px; accent-color: var(--primary); }

    .btn-group { display: flex; gap: 20px; }
    button {
        width: 50px; height: 50px; border-radius: 50%; border: none;
        background: rgba(255,255,255,0.15); color: white; font-size: 20px;
        backdrop-filter: blur(5px); cursor: pointer; transition: 0.2s;
        display: flex; align-items: center; justify-content: center;
        box-shadow: 0 4px 10px rgba(0,0,0,0.3);
    }
    button:active { transform: scale(0.9); background: var(--primary); color: black; }
    button.capture-btn {
        width: 70px; height: 70px; border: 4px solid white; background: transparent;
    }
    button.capture-btn .inner {
        width: 50px; height: 50px; background: white; border-radius: 50%; transition: 0.2s;
    }
    button.capture-btn:active .inner { transform: scale(0.8); background: var(--primary); }

</style>
</head>
<body>

<div id="viewport">
    <div id="zoom-layer">
        <video id="video" autoplay playsinline muted></video>
        <canvas id="canvas"></canvas>
    </div>
</div>

<div id="ui-layer">
    <div id="status" class="status-bar">üöÄ ƒêang kh·ªüi t·∫°o Neural Engine...</div>

    <div class="controls-area">
        <div class="zoom-slider-container">
            <span style="color:#aaa">0.5x</span>
            <input type="range" id="zoomSlider" min="0.5" max="5" step="0.1" value="1">
            <span id="zoomDisplay" class="zoom-val">1.0x</span>
        </div>

        <div class="btn-group">
            <button onclick="switchCamera()">üîÑ</button>
            <button class="capture-btn" onclick="takePhoto()"><div class="inner"></div></button>
            <button onclick="toggleDebug()">üêû</button>
        </div>
    </div>
</div>

<script>
/**
 * LEKIMLAM ARCHITECTURE - CORE CONFIG
 */
const CONFIG = {
    SMOOTH_FACTOR: 0.2, // 0.1 = R·∫•t ch·∫≠m/m∆∞·ª£t, 0.9 = R·∫•t nhanh/rung
    IOU_THRESHOLD: 0.3, // Ng∆∞·ª°ng ƒë·ªÉ coi l√† c√πng m·ªôt v·∫≠t th·ªÉ
    CONFIDENCE: 0.5,    // ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu
    MISS_LIMIT: 10      // S·ªë frame gi·ªØ l·∫°i khung khi m·∫•t d·∫•u
};

// State Management
let state = {
    model: null,
    stream: null,
    facingMode: 'environment',
    zoom: 1,
    tracks: [], // List c√°c v·∫≠t th·ªÉ ƒëang theo d√µi
    isDebug: false,
    videoReady: false
};

const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const statusEl = document.getElementById('status');
const zoomSlider = document.getElementById('zoomSlider');
const zoomLayer = document.getElementById('zoom-layer');

// --- 1. CORE AI LOGIC (SMART TRACKING) ---

// T√≠nh ƒë·ªô ch·ªìng l·∫•n (Intersection over Union)
function getIoU(box1, box2) {
    const [x1, y1, w1, h1] = box1;
    const [x2, y2, w2, h2] = box2;
    
    const xi1 = Math.max(x1, x2);
    const yi1 = Math.max(y1, y2);
    const xi2 = Math.min(x1 + w1, x2 + w2);
    const yi2 = Math.min(y1 + h1, y2 + h2);
    
    const interArea = Math.max(0, xi2 - xi1) * Math.max(0, yi2 - yi1);
    const box1Area = w1 * h1;
    const box2Area = w2 * h2;
    
    return interArea / (box1Area + box2Area - interArea);
}

// Linear Interpolation (L√†m m∆∞·ª£t chuy·ªÉn ƒë·ªông)
function lerp(start, end, amt) {
    return (1 - amt) * start + amt * end;
}

// C·∫≠p nh·∫≠t danh s√°ch v·∫≠t th·ªÉ
function updateTracks(predictions) {
    const newTracks = [];
    
    // ƒê√°nh d·∫•u c√°c track c≈© l√† "ch∆∞a th·∫•y" (unseen)
    state.tracks.forEach(t => t.seen = false);

    predictions.forEach(pred => {
        if (pred.score < CONFIG.CONFIDENCE) return;

        // T√¨m track c≈© kh·ªõp nh·∫•t v·ªõi detection m·ªõi
        let bestMatch = null;
        let maxIoU = 0;

        state.tracks.forEach(track => {
            if (track.class === pred.class) {
                const iou = getIoU(track.box, pred.bbox);
                if (iou > maxIoU) {
                    maxIoU = iou;
                    bestMatch = track;
                }
            }
        });

        if (bestMatch && maxIoU > CONFIG.IOU_THRESHOLD) {
            // C·∫≠p nh·∫≠t track c≈© (Smooth target)
            bestMatch.targetBox = pred.bbox;
            bestMatch.score = pred.score;
            bestMatch.seen = true;
            bestMatch.missFrames = 0;
            newTracks.push(bestMatch);
        } else {
            // T·∫°o track m·ªõi
            newTracks.push({
                id: Math.random().toString(36).substr(2, 9),
                class: pred.class,
                score: pred.score,
                box: pred.bbox, // Box hi·ªán t·∫°i (ƒë·ªÉ v·∫Ω)
                targetBox: pred.bbox, // Box ƒë√≠ch (ƒë·ªÉ lerp t·ªõi)
                seen: true,
                missFrames: 0,
                color: pred.class === 'person' ? '#00ff9d' : '#ff0055'
            });
        }
    });

    // Gi·ªØ l·∫°i c√°c track b·ªã m·∫•t d·∫•u t·∫°m th·ªùi (ch·ªëng nh·∫•p nh√°y khi AI miss 1-2 frame)
    state.tracks.forEach(t => {
        if (!t.seen) {
            t.missFrames++;
            if (t.missFrames < CONFIG.MISS_LIMIT) {
                newTracks.push(t);
            }
        }
    });

    state.tracks = newTracks;
}

async function aiLoop() {
    if (!state.model || !state.videoReady) {
        requestAnimationFrame(aiLoop);
        return;
    }

    // AI ch·∫°y ·ªü t·ªëc ƒë·ªô t·ªëi ƒëa c√≥ th·ªÉ, kh√¥ng ch·∫∑n UI
    try {
        const predictions = await state.model.detect(video);
        updateTracks(predictions);
    } catch (e) {
        console.warn("AI Skip:", e);
    }
    
    requestAnimationFrame(aiLoop); // Loop li√™n t·ª•c
}

// --- 2. RENDERING LOOP (60FPS SMOOTHNESS) ---

function draw() {
    // ƒê·∫£m b·∫£o canvas kh·ªõp k√≠ch th∆∞·ªõc video
    if (canvas.width !== video.videoWidth) {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
    }

    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // V·∫Ω c√°c track
    state.tracks.forEach(track => {
        // Lerp logic: Di chuy·ªÉn box hi·ªán t·∫°i t·ªõi box ƒë√≠ch t·ª´ t·ª´
        track.box[0] = lerp(track.box[0], track.targetBox[0], CONFIG.SMOOTH_FACTOR);
        track.box[1] = lerp(track.box[1], track.targetBox[1], CONFIG.SMOOTH_FACTOR);
        track.box[2] = lerp(track.box[2], track.targetBox[2], CONFIG.SMOOTH_FACTOR);
        track.box[3] = lerp(track.box[3], track.targetBox[3], CONFIG.SMOOTH_FACTOR);

        const [x, y, w, h] = track.box;
        
        // Hi·ªáu ·ª©ng m·ªù d·∫ßn n·∫øu b·ªã m·∫•t d·∫•u
        const opacity = Math.max(0, 1 - (track.missFrames / CONFIG.MISS_LIMIT));
        
        ctx.globalAlpha = opacity;
        
        // V·∫Ω Box
        ctx.strokeStyle = track.color;
        ctx.lineWidth = 3;
        
        // Bo g√≥c cho box (Fancy UI)
        ctx.beginPath();
        ctx.roundRect(x, y, w, h, 8);
        ctx.stroke();

        // V·∫Ω Label n·ªÅn
        ctx.fillStyle = track.color;
        const text = `${track.class} ${Math.round(track.score * 100)}%`;
        const textWidth = ctx.measureText(text).width;
        ctx.fillRect(x, y - 25, textWidth + 10, 25);

        // V·∫Ω Text
        ctx.fillStyle = '#000';
        ctx.font = 'bold 14px Arial';
        ctx.fillText(text, x + 5, y - 7);
        
        ctx.globalAlpha = 1.0;
    });

    if (state.isDebug) {
        statusEl.innerText = `Tracks: ${state.tracks.length} | FPS: ${Math.round(1000/16)}`;
    }

    requestAnimationFrame(draw);
}

// --- 3. SYSTEM & CAMERA ---

async function initCamera() {
    if (state.stream) {
        state.stream.getTracks().forEach(t => t.stop());
    }
    try {
        const constraints = {
            audio: false,
            video: {
                facingMode: state.facingMode,
                width: { ideal: 1280 }, // HD l√† ƒë·ªß cho mobile AI
                height: { ideal: 720 }
            }
        };
        state.stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = state.stream;
        
        video.onloadedmetadata = () => {
            video.play();
            state.videoReady = true;
            statusEl.innerText = "‚úÖ Camera OK - ƒêang t·∫£i AI...";
        };
    } catch (err) {
        statusEl.innerText = "‚ùå L·ªói Camera: " + err.message;
    }
}

async function main() {
    await initCamera();
    
    // Load model
    state.model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
    statusEl.innerText = "‚ö° AI Active - S·∫µn s√†ng";
    
    // Start Loops
    aiLoop(); // Logic AI
    draw();   // Logic V·∫Ω
}

// --- 4. CONTROLS ---

function switchCamera() {
    state.facingMode = state.facingMode === 'user' ? 'environment' : 'user';
    initCamera();
}

function takePhoto() {
    // T·∫°o canvas t·∫°m ƒë·ªÉ crop ƒë√∫ng v√πng zoom
    const tempCanvas = document.createElement('canvas');
    // L·∫•y k√≠ch th∆∞·ªõc th·ª±c t·∫ø hi·ªÉn th·ªã
    const rect = video.getBoundingClientRect();
    const scale = state.zoom;
    
    // Logic ch·ª•p ·∫£nh ƒë∆°n gi·∫£n: ch·ª•p full frame canvas hi·ªán t·∫°i
    // ƒê·ªÉ ch·ª•p ƒë√∫ng zoom c·∫ßn t√≠nh to√°n ph·ª©c t·∫°p h∆°n, ·ªü ƒë√¢y ta ch·ª•p full frame k√®m box
    tempCanvas.width = canvas.width;
    tempCanvas.height = canvas.height;
    const tCtx = tempCanvas.getContext('2d');
    
    tCtx.drawImage(video, 0, 0, canvas.width, canvas.height);
    tCtx.drawImage(canvas, 0, 0); // V·∫Ω ƒë√® box l√™n
    
    const link = document.createElement('a');
    link.download = `AI-Capture-${Date.now()}.png`;
    link.href = tempCanvas.toDataURL();
    link.click();
    
    // Flash effect
    const flash = document.createElement('div');
    flash.style.cssText = 'position:fixed;top:0;left:0;width:100%;height:100%;background:white;z-index:999;transition:opacity 0.3s';
    document.body.appendChild(flash);
    setTimeout(() => { flash.style.opacity = 0; setTimeout(() => flash.remove(), 300); }, 50);
}

function toggleDebug() {
    state.isDebug = !state.isDebug;
    if(!state.isDebug) statusEl.innerText = "‚ö° AI Active - S·∫µn s√†ng";
}

// Zoom Logic
zoomSlider.addEventListener('input', (e) => {
    state.zoom = parseFloat(e.target.value);
    document.getElementById('zoomDisplay').innerText = state.zoom + 'x';
    zoomLayer.style.transform = `scale(${state.zoom})`;
});

// Start
main();

</script>
</body>
</html>    <button onclick="zoomIn()">üîç +</button>
    <button onclick="zoomOut()">üîç -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 5; // TƒÉng ƒë·ªÉ gi·∫£m t·∫£i, smoothing s·∫Ω b√π

// Smoothing params
const SMOOTH_ALPHA = 0.65; // 0.6-0.8: th·∫•p h∆°n = m∆∞·ª£t h∆°n nh∆∞ng ch·∫≠m theo m·ªõi
const MISS_TOLERANCE = 8; // Gi·ªØ box c≈© t·ªëi ƒëa 8 frame n·∫øu miss
const STABILITY_MIN = 3; // Ch·ªâ v·∫Ω n·∫øu ·ªïn ƒë·ªãnh >=3 frame

let trackedObjects = new Map(); // Key: class + approx center, Value: {bbox, score, stableCount, missCount, lastSeen}

function getKey(p) {
    const centerX = Math.round(p.bbox[0] + p.bbox[2]/2);
    const centerY = Math.round(p.bbox[1] + p.bbox[3]/2);
    return `${p.class}-${Math.floor(centerX/20)}-${Math.floor(centerY/20)}`; // Nh√≥m g·∫ßn nhau
}

function smoothAndTrack(newPreds) {
    const currentTime = frameCount;
    const updated = new Map();

    // Process new detections
    newPreds.forEach(p => {
        const key = getKey(p);
        let track = trackedObjects.get(key);

        if (track) {
            // Smooth bbox v·ªõi EMA
            const smoothedBBox = track.bbox.map((old, i) => 
                SMOOTH_ALPHA * p.bbox[i] + (1 - SMOOTH_ALPHA) * old
            );
            track.bbox = smoothedBBox;
            track.score = SMOOTH_ALPHA * p.score + (1 - SMOOTH_ALPHA) * track.score;
            track.stableCount++;
            track.missCount = 0;
            track.lastSeen = currentTime;
            updated.set(key, track);
        } else {
            // New object
            updated.set(key, {
                bbox: [...p.bbox],
                score: p.score,
                class: p.class,
                stableCount: 1,
                missCount: 0,
                lastSeen: currentTime
            });
        }
    });

    // Handle missed detections (gi·ªØ c≈©)
    trackedObjects.forEach((track, key) => {
        if (!updated.has(key)) {
            track.missCount++;
            if (track.missCount <= MISS_TOLERANCE) {
                updated.set(key, track); // Gi·ªØ nguy√™n bbox c≈©
            }
        }
    });

    trackedObjects = updated;

    // Return list to draw: ch·ªâ nh·ªØng ·ªïn ƒë·ªãnh ho·∫∑c score cao
    return Array.from(trackedObjects.values()).filter(t => 
        (t.stableCount >= STABILITY_MIN || t.score > 0.7) && t.missCount < MISS_TOLERANCE
    );
}

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 }
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "üìπ Camera b·∫≠t - Hitbox si√™u ·ªïn ƒë·ªãnh";
    } catch (err) {
        status.innerText = "‚ùå L·ªói camera: " + err.message;
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: currentFacing } });
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const a = document.createElement("a");
    a.download = "ai-photo-on-dinh.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

async function loadAI() {
    status.innerText = "ü§ñ ƒêang t·∫£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "‚úÖ AI s·∫µn s√†ng - Hitbox kh√¥ng nh√°y n·ªØa!";
        detect();
    } catch (err) {
        status.innerText = "‚ùå L·ªói AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filtered = predictions.filter(p => p.score > 0.45);

        const stablePreds = smoothAndTrack(filtered);

        stablePreds.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            // Fade opacity n·∫øu miss nhi·ªÅu (th√™m m∆∞·ª£t)
            const opacity = Math.max(0.3, 1 - (p.missCount / MISS_TOLERANCE));
            ctx.globalAlpha = opacity;

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
            ctx.globalAlpha = 1;
        });

        const personCount = stablePreds.filter(p => p.class === "person").length;
        status.innerText = `üîç ${stablePreds.length} v·∫≠t th·ªÉ ·ªïn ƒë·ªãnh (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x`;
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>    <button onclick="zoomIn()">üîç +</button>
    <button onclick="zoomOut()">üîç -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 4; // TƒÉng ƒë·ªÉ m∆∞·ª£t, gi·∫£m flickering

// Smoothing: L∆∞u predictions tr∆∞·ªõc ƒë·ªÉ blend v√† gi·ªØ ·ªïn ƒë·ªãnh
let prevPredictions = [];
const SMOOTH_ALPHA = 0.7; // 0.7 = ∆∞u ti√™n 70% detections m·ªõi, 30% c≈© (gi·∫£m jitter)
const STABILITY_THRESHOLD = 2; // Ch·ªâ v·∫Ω n·∫øu object xu·∫•t hi·ªán >= 2 frame li√™n ti·∫øp

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 }
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "üìπ Camera b·∫≠t - Box ·ªïn ƒë·ªãnh";
    } catch (err) {
        status.innerText = "‚ùå L·ªói camera: " + err.message;
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: currentFacing } });
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const a = document.createElement("a");
    a.download = "ai-photo-on-dinh.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

function smoothPredictions(newPreds) {
    const smoothed = [];
    const map = new Map(); // Key: class + approx bbox center

    newPreds.forEach(p => {
        const key = `${p.class}-${Math.round(p.bbox[0]/10)}-${Math.round(p.bbox[1]/10)}`;
        map.set(key, p);
    });

    prevPredictions.forEach(old => {
        const key = `${old.class}-${Math.round(old.bbox[0]/10)}-${Math.round(old.bbox[1]/10)}`;
        if (map.has(key)) {
            const newP = map.get(key);
            // Blend bbox
            const blended = [
                SMOOTH_ALPHA * newP.bbox[0] + (1 - SMOOTH_ALPHA) * old.bbox[0],
                SMOOTH_ALPHA * newP.bbox[1] + (1 - SMOOTH_ALPHA) * old.bbox[1],
                SMOOTH_ALPHA * newP.bbox[2] + (1 - SMOOTH_ALPHA) * old.bbox[2],
                SMOOTH_ALPHA * newP.bbox[3] + (1 - SMOOTH_ALPHA) * old.bbox[3]
            ];
            smoothed.push({ ...newP, bbox: blended, stable: true });
            map.delete(key);
        } else {
            // Gi·ªØ old n·∫øu miss detection (gi·∫£m t·∫Øt b·∫≠t)
            smoothed.push({ ...old, stable: old.stable ? old.stable + 1 : 1 });
        }
    });

    // Th√™m new detections m·ªõi
    map.forEach(p => smoothed.push({ ...p, stable: 1 }));

    // Filter ch·ªâ v·∫Ω n·∫øu stable >= threshold
    return smoothed.filter(p => p.stable >= STABILITY_THRESHOLD || p.score > 0.6);
}

async function loadAI() {
    status.innerText = "ü§ñ ƒêang t·∫£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "‚úÖ AI s·∫µn s√†ng - Box gi·ªØ y√™n, kh√¥ng nh√°y!";
        detect();
    } catch (err) {
        status.innerText = "‚ùå L·ªói AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filtered = predictions.filter(p => p.score > 0.4);

        prevPredictions = smoothPredictions(filtered);

        prevPredictions.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
        });

        const personCount = prevPredictions.filter(p => p.class === "person").length;
        status.innerText = `üîç Ph√°t hi·ªán ${prevPredictions.length} v·∫≠t th·ªÉ ·ªïn ƒë·ªãnh (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x`;
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>.controls {
    position: fixed;
    bottom: 80px;
    left: 50%;
    transform: translateX(-50%);
    display: flex;
    gap: 12px;
    z-index: 10;
}

.zoom-controls {
    position: fixed;
    bottom: 120px;
    left: 50%;
    transform: translateX(-50%);
    width: 80%;
    max-width: 300px;
    z-index: 10;
    text-align: center;
    color: #0f0;
}

.zoom-controls label {
    display: block;
    margin-bottom: 5px;
    font-size: 14px;
}

.zoom-slider {
    width: 100%;
    -webkit-appearance: none;
    height: 8px;
    background: rgba(0,255,0,0.3);
    border-radius: 5px;
    outline: none;
}

.zoom-slider::-webkit-slider-thumb {
    -webkit-appearance: none;
    width: 20px;
    height: 20px;
    background: #0f0;
    border-radius: 50%;
    cursor: pointer;
    border: 2px solid #000;
}

button {
    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">üì∑ ƒêang kh·ªüi ƒë·ªông camera...</div>

<div class="controls">
    <button onclick="switchCamera()">üîÅ ƒê·ªïi camera</button>
    <button onclick="takePhoto()">üì∏ Ch·ª•p ·∫£nh</button>
    <button onclick="zoomIn()">üîç +</button>
    <button onclick="zoomOut()">üîç -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 2; // TƒÉng m∆∞·ª£t

const INPUT_SIZE = 640; // YOLOv8 input size
const CONFIDENCE_THRESHOLD = 0.3; // Filter th·∫•p ƒë·ªÉ detect nhi·ªÅu, nh∆∞ng >0.3 ƒë·ªÉ gi·∫£m l·ªói
const IOU_THRESHOLD = 0.45; // NMS threshold

// COCO classes cho YOLOv8 (80 classes)
const CLASS_NAMES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
    "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
    "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
    "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
    "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear",
    "hair drier", "toothbrush"
];

// Sharpen kernel
const sharpenKernel = [0, -1, 0, -1, 5, -1, 0, -1, 0];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale"
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "üìπ Camera ƒë√£ b·∫≠t - Si√™u n√©t";
    } catch (err) {
        status.innerText = "‚ùå L·ªói camera: " + err.message;
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-yolo.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
    updateStatus();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
    updateStatus();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
    updateStatus();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => CLASS_NAMES[p.class] === "person").length;
    status.innerText = `üîç Ph√°t hi·ªán ${predictions.length} v·∫≠t th·ªÉ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

async function loadAI() {
    status.innerText = "ü§ñ ƒêang t·∫£i YOLOv8 si√™u t·ªëi t√¢n...";
    try {
        model = await tf.loadGraphModel("./yolov8n_web_model/model.json"); // Ch·ªânh n·∫øu d√πng yolov8s
        status.innerText = "‚úÖ YOLOv8 s·∫µn s√†ng - Detect si√™u ch√≠nh x√°c!";
        detect();
    } catch (err) {
        status.innerText = "‚ùå L·ªói t·∫£i YOLOv8: " + err.message + " (ki·ªÉm tra folder model)";
    }
}

// Preprocess input cho YOLOv8
function preprocess(src) {
    let [input, xRatio, yRatio] = tf.tidy(() => {
        const img = tf.browser.fromPixels(src);
        const [h, w] = img.shape.slice(0, 2);
        const maxSize = Math.max(h, w);
        const imgPadded = img.pad([[0, maxSize - h], [0, maxSize - w], [0, 0]]);
        return [
            tf.expandDims(tf.div(tf.image.resizeBilinear(imgPadded, [INPUT_SIZE, INPUT_SIZE]), 255.0), 0),
            w / maxSize,
            h / maxSize
        ];
    });
    return [input, xRatio, yRatio];
}

// Postprocess output YOLOv8
function postprocess(output, xRatio, yRatio) {
    return tf.tidy(() => {
        const boxes = [];
        output = output.transpose([0, 2, 1]); // [1, 8400, 84] -> [1, 84, 8400] nh∆∞ng adjust cho YOLOv8
        output = output.squeeze(); // [84, 8400]
        const [classProbs, boxesData] = tf.split(output, [80, 4], 0); // Classes v√† boxes
        const scores = tf.max(classProbs, 0).dataSync();
        const classes = tf.argMax(classProbs, 0).dataSync();

        const rawBoxes = boxesData.transpose().dataSync();

        for (let i = 0; i < scores.length; ++i) {
            if (scores[i] > CONFIDENCE_THRESHOLD) {
                const [yCenter, xCenter, height, width] = rawBoxes.slice(i * 4, (i + 1) * 4);
                const top = (yCenter - height / 2) / yRatio;
                const left = (xCenter - width / 2) / xRatio;
                const bottom = (yCenter + height / 2) / yRatio;
                const right = (xCenter + width / 2) / xRatio;
                boxes.push({
                    top: Math.max(0, top),
                    left: Math.max(0, left),
                    bottom: Math.min(video.videoHeight, bottom),
                    right: Math.min(video.videoWidth, right),
                    class: classes[i],
                    score: scores[i]
                });
            }
        }

        // NMS ƒë·ªÉ gi·∫£m tr√πng l·∫∑p
        const selected = tf.image.nonMaxSuppression(
            boxes.map(item => [item.top, item.left, item.bottom, item.right]),
            boxes.map(item => item.score),
            20, // Max output
            IOU_THRESHOLD,
            CONFIDENCE_THRESHOLD
        ).dataSync();

        return selected.map(index => boxes[index]);
    });
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;
            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }
            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen(); // Sharpen m·ªói frame

    frameCount++;
    if (frameCount % detectInterval === 0) {
        tf.tidy(() => {
            const [input, xRatio, yRatio] = preprocess(video);
            const res = model.execute(input);
            const predictions = postprocess(res, xRatio, yRatio);

            predictions.forEach(p => {
                const x = p.left;
                const y = p.top;
                const w = p.right - p.left;
                const h = p.bottom - p.top;
                const className = CLASS_NAMES[p.class];
                const isPerson = className === "person";

                ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.lineWidth = isPerson ? 4 : 2;
                ctx.strokeRect(x, y, w, h);

                ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.font = isPerson ? "18px Arial" : "14px Arial";
                ctx.fillText(
                    `${className} (${Math.round(p.score * 100)}%)`,
                    x,
                    y > 25 ? y - 8 : y + 18
                );
            });

            updateStatus(predictions);
        });
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">üì∑ ƒêang kh·ªüi ƒë·ªông camera...</div>

<div class="controls">
    <button onclick="switchCamera()">üîÅ ƒê·ªïi camera</button>
    <button onclick="takePhoto()">üì∏ Ch·ª•p ·∫£nh</button>
    <button onclick="zoomIn()">üîç +</button>
    <button onclick="zoomOut()">üîç -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 3;

// Sharpen kernel (nh·∫π, kh√¥ng qu√° m·∫°nh ƒë·ªÉ tr√°nh noise)
const sharpenKernel = [
    0, -1, 0,
    -1, 5, -1,
    0, -1, 0
];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        // Constraints cho r√µ n√©t cao: ∆∞u ti√™n high-res, rear camera
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920, min: 1280 },
                height: { ideal: 1080, min: 720 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale" // Gi·ªØ ch·∫•t l∆∞·ª£ng cao khi scale
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "üìπ Camera ƒë√£ b·∫≠t - Ch·∫•t l∆∞·ª£ng cao";
    } catch (err) {
        status.innerText = "‚ùå L·ªói camera: " + err.message + " (th·ª≠ gi·∫£m res n·∫øu l·ªói)";
        // Fallback low res n·∫øu high fail
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    // √Åp sharpen khi ch·ª•p ƒë·ªÉ ·∫£nh n√©t h∆°n
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-net.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => p.class === "person" && p.score > 0.5).length;
    status.innerText = `üîç Ph√°t hi·ªán ${predictions.length} v·∫≠t th·ªÉ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;

            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }

            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
            data[i + 3] = 255;
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function loadAI() {
    status.innerText = "ü§ñ ƒêang t·∫£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "‚úÖ AI s·∫µn s√†ng";
        detect();
    } catch (err) {
        status.innerText = "‚ùå L·ªói AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // √Åp sharpen nh·∫π l√™n frame ƒë·ªÉ r√µ n√©t h∆°n (m·ªói frame)
    applySharpen();

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filteredPreds = predictions.filter(p => p.score > 0.5);

        // V·∫Ω bounding box sau sharpen
        filteredPreds.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
        });

        updateStatus(filteredPreds);
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>
