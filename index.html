<!DOCTYPE html>
<html lang="vi">
<head>
<meta charset="UTF-8">
<title>Camera AI PhÃ¡t Hiá»‡n Váº­t Thá»ƒ SiÃªu Tá»‘i TÃ¢n (YOLOv8)</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>

<style>
body {
    margin: 0;
    background: black;
    overflow: hidden;
    font-family: Arial;
}

#container {
    position: relative;
    width: 100vw;
    height: 100vh;
    overflow: hidden;
}

video, canvas {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    width: 100%;
    height: 100%;
    object-fit: cover;
    transition: transform 0.3s ease;
    image-rendering: crisp-edges;
}

.controls {
    position: fixed;
    bottom: 80px;
    left: 50%;
    transform: translateX(-50%);
    display: flex;
    gap: 12px;
    z-index: 10;
}

.zoom-controls {
    position: fixed;
    bottom: 120px;
    left: 50%;
    transform: translateX(-50%);
    width: 80%;
    max-width: 300px;
    z-index: 10;
    text-align: center;
    color: #0f0;
}

.zoom-controls label {
    display: block;
    margin-bottom: 5px;
    font-size: 14px;
}

.zoom-slider {
    width: 100%;
    -webkit-appearance: none;
    height: 8px;
    background: rgba(0,255,0,0.3);
    border-radius: 5px;
    outline: none;
}

.zoom-slider::-webkit-slider-thumb {
    -webkit-appearance: none;
    width: 20px;
    height: 20px;
    background: #0f0;
    border-radius: 50%;
    cursor: pointer;
    border: 2px solid #000;
}

button {
    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">ğŸ“· Äang khá»Ÿi Ä‘á»™ng camera...</div>

<div class="controls">
    <button onclick="switchCamera()">ğŸ” Äá»•i camera</button>
    <button onclick="takePhoto()">ğŸ“¸ Chá»¥p áº£nh</button>
    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 2; // TÄƒng mÆ°á»£t

const INPUT_SIZE = 640; // YOLOv8 input size
const CONFIDENCE_THRESHOLD = 0.3; // Filter tháº¥p Ä‘á»ƒ detect nhiá»u, nhÆ°ng >0.3 Ä‘á»ƒ giáº£m lá»—i
const IOU_THRESHOLD = 0.45; // NMS threshold

// COCO classes cho YOLOv8 (80 classes)
const CLASS_NAMES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
    "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
    "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
    "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
    "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear",
    "hair drier", "toothbrush"
];

// Sharpen kernel
const sharpenKernel = [0, -1, 0, -1, 5, -1, 0, -1, 0];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale"
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera Ä‘Ã£ báº­t - SiÃªu nÃ©t";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message;
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-yolo.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
    updateStatus();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
    updateStatus();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
    updateStatus();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => CLASS_NAMES[p.class] === "person").length;
    status.innerText = `ğŸ” PhÃ¡t hiá»‡n ${predictions.length} váº­t thá»ƒ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i YOLOv8 siÃªu tá»‘i tÃ¢n...";
    try {
        model = await tf.loadGraphModel("./yolov8n_web_model/model.json"); // Chá»‰nh náº¿u dÃ¹ng yolov8s
        status.innerText = "âœ… YOLOv8 sáºµn sÃ ng - Detect siÃªu chÃ­nh xÃ¡c!";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i táº£i YOLOv8: " + err.message + " (kiá»ƒm tra folder model)";
    }
}

// Preprocess input cho YOLOv8
function preprocess(src) {
    let [input, xRatio, yRatio] = tf.tidy(() => {
        const img = tf.browser.fromPixels(src);
        const [h, w] = img.shape.slice(0, 2);
        const maxSize = Math.max(h, w);
        const imgPadded = img.pad([[0, maxSize - h], [0, maxSize - w], [0, 0]]);
        return [
            tf.expandDims(tf.div(tf.image.resizeBilinear(imgPadded, [INPUT_SIZE, INPUT_SIZE]), 255.0), 0),
            w / maxSize,
            h / maxSize
        ];
    });
    return [input, xRatio, yRatio];
}

// Postprocess output YOLOv8
function postprocess(output, xRatio, yRatio) {
    return tf.tidy(() => {
        const boxes = [];
        output = output.transpose([0, 2, 1]); // [1, 8400, 84] -> [1, 84, 8400] nhÆ°ng adjust cho YOLOv8
        output = output.squeeze(); // [84, 8400]
        const [classProbs, boxesData] = tf.split(output, [80, 4], 0); // Classes vÃ  boxes
        const scores = tf.max(classProbs, 0).dataSync();
        const classes = tf.argMax(classProbs, 0).dataSync();

        const rawBoxes = boxesData.transpose().dataSync();

        for (let i = 0; i < scores.length; ++i) {
            if (scores[i] > CONFIDENCE_THRESHOLD) {
                const [yCenter, xCenter, height, width] = rawBoxes.slice(i * 4, (i + 1) * 4);
                const top = (yCenter - height / 2) / yRatio;
                const left = (xCenter - width / 2) / xRatio;
                const bottom = (yCenter + height / 2) / yRatio;
                const right = (xCenter + width / 2) / xRatio;
                boxes.push({
                    top: Math.max(0, top),
                    left: Math.max(0, left),
                    bottom: Math.min(video.videoHeight, bottom),
                    right: Math.min(video.videoWidth, right),
                    class: classes[i],
                    score: scores[i]
                });
            }
        }

        // NMS Ä‘á»ƒ giáº£m trÃ¹ng láº·p
        const selected = tf.image.nonMaxSuppression(
            boxes.map(item => [item.top, item.left, item.bottom, item.right]),
            boxes.map(item => item.score),
            20, // Max output
            IOU_THRESHOLD,
            CONFIDENCE_THRESHOLD
        ).dataSync();

        return selected.map(index => boxes[index]);
    });
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;
            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }
            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen(); // Sharpen má»—i frame

    frameCount++;
    if (frameCount % detectInterval === 0) {
        tf.tidy(() => {
            const [input, xRatio, yRatio] = preprocess(video);
            const res = model.execute(input);
            const predictions = postprocess(res, xRatio, yRatio);

            predictions.forEach(p => {
                const x = p.left;
                const y = p.top;
                const w = p.right - p.left;
                const h = p.bottom - p.top;
                const className = CLASS_NAMES[p.class];
                const isPerson = className === "person";

                ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.lineWidth = isPerson ? 4 : 2;
                ctx.strokeRect(x, y, w, h);

                ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.font = isPerson ? "18px Arial" : "14px Arial";
                ctx.fillText(
                    `${className} (${Math.round(p.score * 100)}%)`,
                    x,
                    y > 25 ? y - 8 : y + 18
                );
            });

            updateStatus(predictions);
        });
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">ğŸ“· Äang khá»Ÿi Ä‘á»™ng camera...</div>

<div class="controls">
    <button onclick="switchCamera()">ğŸ” Äá»•i camera</button>
    <button onclick="takePhoto()">ğŸ“¸ Chá»¥p áº£nh</button>
    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 3;

// Sharpen kernel (nháº¹, khÃ´ng quÃ¡ máº¡nh Ä‘á»ƒ trÃ¡nh noise)
const sharpenKernel = [
    0, -1, 0,
    -1, 5, -1,
    0, -1, 0
];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        // Constraints cho rÃµ nÃ©t cao: Æ°u tiÃªn high-res, rear camera
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920, min: 1280 },
                height: { ideal: 1080, min: 720 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale" // Giá»¯ cháº¥t lÆ°á»£ng cao khi scale
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera Ä‘Ã£ báº­t - Cháº¥t lÆ°á»£ng cao";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message + " (thá»­ giáº£m res náº¿u lá»—i)";
        // Fallback low res náº¿u high fail
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    // Ãp sharpen khi chá»¥p Ä‘á»ƒ áº£nh nÃ©t hÆ¡n
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-net.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => p.class === "person" && p.score > 0.5).length;
    status.innerText = `ğŸ” PhÃ¡t hiá»‡n ${predictions.length} váº­t thá»ƒ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;

            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }

            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
            data[i + 3] = 255;
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "âœ… AI sáºµn sÃ ng";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Ãp sharpen nháº¹ lÃªn frame Ä‘á»ƒ rÃµ nÃ©t hÆ¡n (má»—i frame)
    applySharpen();

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filteredPreds = predictions.filter(p => p.score > 0.5);

        // Váº½ bounding box sau sharpen
        filteredPreds.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
        });

        updateStatus(filteredPreds);
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>
