<!DOCTYPE html>
<html lang="vi">
<head>
<meta charset="UTF-8">
<title>LeKimLam AI Camera - Ultra Stable</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<!-- TensorFlow.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<style>
    :root { --primary: #00ff9d; --bg-glass: rgba(20, 20, 20, 0.6); }
    body { margin: 0; background: #000; overflow: hidden; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; }
    
    /* Container xá»­ lÃ½ Zoom */
    #viewport { position: relative; width: 100vw; height: 100vh; overflow: hidden; }
    #zoom-layer { 
        position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
        transform-origin: center center; transition: transform 0.1s linear;
        display: flex; align-items: center; justify-content: center;
    }

    /* Video & Canvas xáº¿p chá»“ng khÃ­t nhau */
    video { position: absolute; width: 100%; height: 100%; object-fit: cover; }
    canvas { position: absolute; width: 100%; height: 100%; object-fit: cover; pointer-events: none; }

    /* UI Layer - Náº±m trÃªn cÃ¹ng, khÃ´ng bá»‹ Zoom */
    #ui-layer { position: fixed; top: 0; left: 0; width: 100%; height: 100%; pointer-events: none; z-index: 100; }
    
    .status-bar {
        position: absolute; top: 20px; left: 50%; transform: translateX(-50%);
        background: var(--bg-glass); backdrop-filter: blur(10px);
        padding: 8px 16px; border-radius: 20px; color: var(--primary);
        font-size: 13px; font-weight: 600; text-align: center;
        border: 1px solid rgba(0, 255, 157, 0.2); pointer-events: auto;
    }

    .controls-area {
        position: absolute; bottom: 30px; left: 0; width: 100%;
        display: flex; flex-direction: column; align-items: center; gap: 15px;
        pointer-events: auto;
    }

    .zoom-slider-container {
        display: flex; align-items: center; gap: 10px;
        background: var(--bg-glass); padding: 10px 20px; border-radius: 30px;
        backdrop-filter: blur(5px);
    }
    .zoom-val { color: white; min-width: 40px; text-align: center; font-variant-numeric: tabular-nums; }
    input[type=range] { width: 150px; accent-color: var(--primary); }

    .btn-group { display: flex; gap: 20px; }
    button {
        width: 50px; height: 50px; border-radius: 50%; border: none;
        background: rgba(255,255,255,0.15); color: white; font-size: 20px;
        backdrop-filter: blur(5px); cursor: pointer; transition: 0.2s;
        display: flex; align-items: center; justify-content: center;
        box-shadow: 0 4px 10px rgba(0,0,0,0.3);
    }
    button:active { transform: scale(0.9); background: var(--primary); color: black; }
    button.capture-btn {
        width: 70px; height: 70px; border: 4px solid white; background: transparent;
    }
    button.capture-btn .inner {
        width: 50px; height: 50px; background: white; border-radius: 50%; transition: 0.2s;
    }
    button.capture-btn:active .inner { transform: scale(0.8); background: var(--primary); }

</style>
</head>
<body>

<div id="viewport">
    <div id="zoom-layer">
        <video id="video" autoplay playsinline muted></video>
        <canvas id="canvas"></canvas>
    </div>
</div>

<div id="ui-layer">
    <div id="status" class="status-bar">ğŸš€ Äang khá»Ÿi táº¡o Neural Engine...</div>

    <div class="controls-area">
        <div class="zoom-slider-container">
            <span style="color:#aaa">0.5x</span>
            <input type="range" id="zoomSlider" min="0.5" max="5" step="0.1" value="1">
            <span id="zoomDisplay" class="zoom-val">1.0x</span>
        </div>

        <div class="btn-group">
            <button onclick="switchCamera()">ğŸ”„</button>
            <button class="capture-btn" onclick="takePhoto()"><div class="inner"></div></button>
            <button onclick="toggleDebug()">ğŸ</button>
        </div>
    </div>
</div>

<script>
/**
 * LEKIMLAM ARCHITECTURE - CORE CONFIG
 */
const CONFIG = {
    SMOOTH_FACTOR: 0.2, // 0.1 = Ráº¥t cháº­m/mÆ°á»£t, 0.9 = Ráº¥t nhanh/rung
    IOU_THRESHOLD: 0.3, // NgÆ°á»¡ng Ä‘á»ƒ coi lÃ  cÃ¹ng má»™t váº­t thá»ƒ
    CONFIDENCE: 0.5,    // Äá»™ tin cáº­y tá»‘i thiá»ƒu
    MISS_LIMIT: 10      // Sá»‘ frame giá»¯ láº¡i khung khi máº¥t dáº¥u
};

// State Management
let state = {
    model: null,
    stream: null,
    facingMode: 'environment',
    zoom: 1,
    tracks: [], // List cÃ¡c váº­t thá»ƒ Ä‘ang theo dÃµi
    isDebug: false,
    videoReady: false
};

const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const statusEl = document.getElementById('status');
const zoomSlider = document.getElementById('zoomSlider');
const zoomLayer = document.getElementById('zoom-layer');

// --- 1. CORE AI LOGIC (SMART TRACKING) ---

// TÃ­nh Ä‘á»™ chá»“ng láº¥n (Intersection over Union)
function getIoU(box1, box2) {
    const [x1, y1, w1, h1] = box1;
    const [x2, y2, w2, h2] = box2;
    
    const xi1 = Math.max(x1, x2);
    const yi1 = Math.max(y1, y2);
    const xi2 = Math.min(x1 + w1, x2 + w2);
    const yi2 = Math.min(y1 + h1, y2 + h2);
    
    const interArea = Math.max(0, xi2 - xi1) * Math.max(0, yi2 - yi1);
    const box1Area = w1 * h1;
    const box2Area = w2 * h2;
    
    return interArea / (box1Area + box2Area - interArea);
}

// Linear Interpolation (LÃ m mÆ°á»£t chuyá»ƒn Ä‘á»™ng)
function lerp(start, end, amt) {
    return (1 - amt) * start + amt * end;
}

// Cáº­p nháº­t danh sÃ¡ch váº­t thá»ƒ
function updateTracks(predictions) {
    const newTracks = [];
    
    // ÄÃ¡nh dáº¥u cÃ¡c track cÅ© lÃ  "chÆ°a tháº¥y" (unseen)
    state.tracks.forEach(t => t.seen = false);

    predictions.forEach(pred => {
        if (pred.score < CONFIG.CONFIDENCE) return;

        // TÃ¬m track cÅ© khá»›p nháº¥t vá»›i detection má»›i
        let bestMatch = null;
        let maxIoU = 0;

        state.tracks.forEach(track => {
            if (track.class === pred.class) {
                const iou = getIoU(track.box, pred.bbox);
                if (iou > maxIoU) {
                    maxIoU = iou;
                    bestMatch = track;
                }
            }
        });

        if (bestMatch && maxIoU > CONFIG.IOU_THRESHOLD) {
            // Cáº­p nháº­t track cÅ© (Smooth target)
            bestMatch.targetBox = pred.bbox;
            bestMatch.score = pred.score;
            bestMatch.seen = true;
            bestMatch.missFrames = 0;
            newTracks.push(bestMatch);
        } else {
            // Táº¡o track má»›i
            newTracks.push({
                id: Math.random().toString(36).substr(2, 9),
                class: pred.class,
                score: pred.score,
                box: pred.bbox, // Box hiá»‡n táº¡i (Ä‘á»ƒ váº½)
                targetBox: pred.bbox, // Box Ä‘Ã­ch (Ä‘á»ƒ lerp tá»›i)
                seen: true,
                missFrames: 0,
                color: pred.class === 'person' ? '#00ff9d' : '#ff0055'
            });
        }
    });

    // Giá»¯ láº¡i cÃ¡c track bá»‹ máº¥t dáº¥u táº¡m thá»i (chá»‘ng nháº¥p nhÃ¡y khi AI miss 1-2 frame)
    state.tracks.forEach(t => {
        if (!t.seen) {
            t.missFrames++;
            if (t.missFrames < CONFIG.MISS_LIMIT) {
                newTracks.push(t);
            }
        }
    });

    state.tracks = newTracks;
}

async function aiLoop() {
    if (!state.model || !state.videoReady) {
        requestAnimationFrame(aiLoop);
        return;
    }

    // AI cháº¡y á»Ÿ tá»‘c Ä‘á»™ tá»‘i Ä‘a cÃ³ thá»ƒ, khÃ´ng cháº·n UI
    try {
        const predictions = await state.model.detect(video);
        updateTracks(predictions);
    } catch (e) {
        console.warn("AI Skip:", e);
    }
    
    requestAnimationFrame(aiLoop); // Loop liÃªn tá»¥c
}

// --- 2. RENDERING LOOP (60FPS SMOOTHNESS) ---

function draw() {
    // Äáº£m báº£o canvas khá»›p kÃ­ch thÆ°á»›c video
    if (canvas.width !== video.videoWidth) {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
    }

    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // Váº½ cÃ¡c track
    state.tracks.forEach(track => {
        // Lerp logic: Di chuyá»ƒn box hiá»‡n táº¡i tá»›i box Ä‘Ã­ch tá»« tá»«
        track.box[0] = lerp(track.box[0], track.targetBox[0], CONFIG.SMOOTH_FACTOR);
        track.box[1] = lerp(track.box[1], track.targetBox[1], CONFIG.SMOOTH_FACTOR);
        track.box[2] = lerp(track.box[2], track.targetBox[2], CONFIG.SMOOTH_FACTOR);
        track.box[3] = lerp(track.box[3], track.targetBox[3], CONFIG.SMOOTH_FACTOR);

        const [x, y, w, h] = track.box;
        
        // Hiá»‡u á»©ng má» dáº§n náº¿u bá»‹ máº¥t dáº¥u
        const opacity = Math.max(0, 1 - (track.missFrames / CONFIG.MISS_LIMIT));
        
        ctx.globalAlpha = opacity;
        
        // Váº½ Box
        ctx.strokeStyle = track.color;
        ctx.lineWidth = 3;
        
        // Bo gÃ³c cho box (Fancy UI)
        ctx.beginPath();
        ctx.roundRect(x, y, w, h, 8);
        ctx.stroke();

        // Váº½ Label ná»n
        ctx.fillStyle = track.color;
        const text = `${track.class} ${Math.round(track.score * 100)}%`;
        const textWidth = ctx.measureText(text).width;
        ctx.fillRect(x, y - 25, textWidth + 10, 25);

        // Váº½ Text
        ctx.fillStyle = '#000';
        ctx.font = 'bold 14px Arial';
        ctx.fillText(text, x + 5, y - 7);
        
        ctx.globalAlpha = 1.0;
    });

    if (state.isDebug) {
        statusEl.innerText = `Tracks: ${state.tracks.length} | FPS: ${Math.round(1000/16)}`;
    }

    requestAnimationFrame(draw);
}

// --- 3. SYSTEM & CAMERA ---

async function initCamera() {
    if (state.stream) {
        state.stream.getTracks().forEach(t => t.stop());
    }
    try {
        const constraints = {
            audio: false,
            video: {
                facingMode: state.facingMode,
                width: { ideal: 1280 }, // HD lÃ  Ä‘á»§ cho mobile AI
                height: { ideal: 720 }
            }
        };
        state.stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = state.stream;
        
        video.onloadedmetadata = () => {
            video.play();
            state.videoReady = true;
            statusEl.innerText = "âœ… Camera OK - Äang táº£i AI...";
        };
    } catch (err) {
        statusEl.innerText = "âŒ Lá»—i Camera: " + err.message;
    }
}

async function main() {
    await initCamera();
    
    // Load model
    state.model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
    statusEl.innerText = "âš¡ AI Active - Sáºµn sÃ ng";
    
    // Start Loops
    aiLoop(); // Logic AI
    draw();   // Logic Váº½
}

// --- 4. CONTROLS ---

function switchCamera() {
    state.facingMode = state.facingMode === 'user' ? 'environment' : 'user';
    initCamera();
}

function takePhoto() {
    // Táº¡o canvas táº¡m Ä‘á»ƒ crop Ä‘Ãºng vÃ¹ng zoom
    const tempCanvas = document.createElement('canvas');
    // Láº¥y kÃ­ch thÆ°á»›c thá»±c táº¿ hiá»ƒn thá»‹
    const rect = video.getBoundingClientRect();
    const scale = state.zoom;
    
    // Logic chá»¥p áº£nh Ä‘Æ¡n giáº£n: chá»¥p full frame canvas hiá»‡n táº¡i
    // Äá»ƒ chá»¥p Ä‘Ãºng zoom cáº§n tÃ­nh toÃ¡n phá»©c táº¡p hÆ¡n, á»Ÿ Ä‘Ã¢y ta chá»¥p full frame kÃ¨m box
    tempCanvas.width = canvas.width;
    tempCanvas.height = canvas.height;
    const tCtx = tempCanvas.getContext('2d');
    
    tCtx.drawImage(video, 0, 0, canvas.width, canvas.height);
    tCtx.drawImage(canvas, 0, 0); // Váº½ Ä‘Ã¨ box lÃªn
    
    const link = document.createElement('a');
    link.download = `AI-Capture-${Date.now()}.png`;
    link.href = tempCanvas.toDataURL();
    link.click();
    
    // Flash effect
    const flash = document.createElement('div');
    flash.style.cssText = 'position:fixed;top:0;left:0;width:100%;height:100%;background:white;z-index:999;transition:opacity 0.3s';
    document.body.appendChild(flash);
    setTimeout(() => { flash.style.opacity = 0; setTimeout(() => flash.remove(), 300); }, 50);
}

function toggleDebug() {
    state.isDebug = !state.isDebug;
    if(!state.isDebug) statusEl.innerText = "âš¡ AI Active - Sáºµn sÃ ng";
}

// Zoom Logic
zoomSlider.addEventListener('input', (e) => {
    state.zoom = parseFloat(e.target.value);
    document.getElementById('zoomDisplay').innerText = state.zoom + 'x';
    zoomLayer.style.transform = `scale(${state.zoom})`;
});

// Start
main();

</script>
</body>
</html>    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 5; // TÄƒng Ä‘á»ƒ giáº£m táº£i, smoothing sáº½ bÃ¹

// Smoothing params
const SMOOTH_ALPHA = 0.65; // 0.6-0.8: tháº¥p hÆ¡n = mÆ°á»£t hÆ¡n nhÆ°ng cháº­m theo má»›i
const MISS_TOLERANCE = 8; // Giá»¯ box cÅ© tá»‘i Ä‘a 8 frame náº¿u miss
const STABILITY_MIN = 3; // Chá»‰ váº½ náº¿u á»•n Ä‘á»‹nh >=3 frame

let trackedObjects = new Map(); // Key: class + approx center, Value: {bbox, score, stableCount, missCount, lastSeen}

function getKey(p) {
    const centerX = Math.round(p.bbox[0] + p.bbox[2]/2);
    const centerY = Math.round(p.bbox[1] + p.bbox[3]/2);
    return `${p.class}-${Math.floor(centerX/20)}-${Math.floor(centerY/20)}`; // NhÃ³m gáº§n nhau
}

function smoothAndTrack(newPreds) {
    const currentTime = frameCount;
    const updated = new Map();

    // Process new detections
    newPreds.forEach(p => {
        const key = getKey(p);
        let track = trackedObjects.get(key);

        if (track) {
            // Smooth bbox vá»›i EMA
            const smoothedBBox = track.bbox.map((old, i) => 
                SMOOTH_ALPHA * p.bbox[i] + (1 - SMOOTH_ALPHA) * old
            );
            track.bbox = smoothedBBox;
            track.score = SMOOTH_ALPHA * p.score + (1 - SMOOTH_ALPHA) * track.score;
            track.stableCount++;
            track.missCount = 0;
            track.lastSeen = currentTime;
            updated.set(key, track);
        } else {
            // New object
            updated.set(key, {
                bbox: [...p.bbox],
                score: p.score,
                class: p.class,
                stableCount: 1,
                missCount: 0,
                lastSeen: currentTime
            });
        }
    });

    // Handle missed detections (giá»¯ cÅ©)
    trackedObjects.forEach((track, key) => {
        if (!updated.has(key)) {
            track.missCount++;
            if (track.missCount <= MISS_TOLERANCE) {
                updated.set(key, track); // Giá»¯ nguyÃªn bbox cÅ©
            }
        }
    });

    trackedObjects = updated;

    // Return list to draw: chá»‰ nhá»¯ng á»•n Ä‘á»‹nh hoáº·c score cao
    return Array.from(trackedObjects.values()).filter(t => 
        (t.stableCount >= STABILITY_MIN || t.score > 0.7) && t.missCount < MISS_TOLERANCE
    );
}

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 }
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera báº­t - Hitbox siÃªu á»•n Ä‘á»‹nh";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message;
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: currentFacing } });
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const a = document.createElement("a");
    a.download = "ai-photo-on-dinh.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "âœ… AI sáºµn sÃ ng - Hitbox khÃ´ng nhÃ¡y ná»¯a!";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filtered = predictions.filter(p => p.score > 0.45);

        const stablePreds = smoothAndTrack(filtered);

        stablePreds.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            // Fade opacity náº¿u miss nhiá»u (thÃªm mÆ°á»£t)
            const opacity = Math.max(0.3, 1 - (p.missCount / MISS_TOLERANCE));
            ctx.globalAlpha = opacity;

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
            ctx.globalAlpha = 1;
        });

        const personCount = stablePreds.filter(p => p.class === "person").length;
        status.innerText = `ğŸ” ${stablePreds.length} váº­t thá»ƒ á»•n Ä‘á»‹nh (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x`;
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 4; // TÄƒng Ä‘á»ƒ mÆ°á»£t, giáº£m flickering

// Smoothing: LÆ°u predictions trÆ°á»›c Ä‘á»ƒ blend vÃ  giá»¯ á»•n Ä‘á»‹nh
let prevPredictions = [];
const SMOOTH_ALPHA = 0.7; // 0.7 = Æ°u tiÃªn 70% detections má»›i, 30% cÅ© (giáº£m jitter)
const STABILITY_THRESHOLD = 2; // Chá»‰ váº½ náº¿u object xuáº¥t hiá»‡n >= 2 frame liÃªn tiáº¿p

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 }
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera báº­t - Box á»•n Ä‘á»‹nh";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message;
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: currentFacing } });
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const a = document.createElement("a");
    a.download = "ai-photo-on-dinh.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

function smoothPredictions(newPreds) {
    const smoothed = [];
    const map = new Map(); // Key: class + approx bbox center

    newPreds.forEach(p => {
        const key = `${p.class}-${Math.round(p.bbox[0]/10)}-${Math.round(p.bbox[1]/10)}`;
        map.set(key, p);
    });

    prevPredictions.forEach(old => {
        const key = `${old.class}-${Math.round(old.bbox[0]/10)}-${Math.round(old.bbox[1]/10)}`;
        if (map.has(key)) {
            const newP = map.get(key);
            // Blend bbox
            const blended = [
                SMOOTH_ALPHA * newP.bbox[0] + (1 - SMOOTH_ALPHA) * old.bbox[0],
                SMOOTH_ALPHA * newP.bbox[1] + (1 - SMOOTH_ALPHA) * old.bbox[1],
                SMOOTH_ALPHA * newP.bbox[2] + (1 - SMOOTH_ALPHA) * old.bbox[2],
                SMOOTH_ALPHA * newP.bbox[3] + (1 - SMOOTH_ALPHA) * old.bbox[3]
            ];
            smoothed.push({ ...newP, bbox: blended, stable: true });
            map.delete(key);
        } else {
            // Giá»¯ old náº¿u miss detection (giáº£m táº¯t báº­t)
            smoothed.push({ ...old, stable: old.stable ? old.stable + 1 : 1 });
        }
    });

    // ThÃªm new detections má»›i
    map.forEach(p => smoothed.push({ ...p, stable: 1 }));

    // Filter chá»‰ váº½ náº¿u stable >= threshold
    return smoothed.filter(p => p.stable >= STABILITY_THRESHOLD || p.score > 0.6);
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "âœ… AI sáºµn sÃ ng - Box giá»¯ yÃªn, khÃ´ng nhÃ¡y!";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filtered = predictions.filter(p => p.score > 0.4);

        prevPredictions = smoothPredictions(filtered);

        prevPredictions.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
        });

        const personCount = prevPredictions.filter(p => p.class === "person").length;
        status.innerText = `ğŸ” PhÃ¡t hiá»‡n ${prevPredictions.length} váº­t thá»ƒ á»•n Ä‘á»‹nh (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x`;
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>.controls {
    position: fixed;
    bottom: 80px;
    left: 50%;
    transform: translateX(-50%);
    display: flex;
    gap: 12px;
    z-index: 10;
}

.zoom-controls {
    position: fixed;
    bottom: 120px;
    left: 50%;
    transform: translateX(-50%);
    width: 80%;
    max-width: 300px;
    z-index: 10;
    text-align: center;
    color: #0f0;
}

.zoom-controls label {
    display: block;
    margin-bottom: 5px;
    font-size: 14px;
}

.zoom-slider {
    width: 100%;
    -webkit-appearance: none;
    height: 8px;
    background: rgba(0,255,0,0.3);
    border-radius: 5px;
    outline: none;
}

.zoom-slider::-webkit-slider-thumb {
    -webkit-appearance: none;
    width: 20px;
    height: 20px;
    background: #0f0;
    border-radius: 50%;
    cursor: pointer;
    border: 2px solid #000;
}

button {
    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">ğŸ“· Äang khá»Ÿi Ä‘á»™ng camera...</div>

<div class="controls">
    <button onclick="switchCamera()">ğŸ” Äá»•i camera</button>
    <button onclick="takePhoto()">ğŸ“¸ Chá»¥p áº£nh</button>
    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 2; // TÄƒng mÆ°á»£t

const INPUT_SIZE = 640; // YOLOv8 input size
const CONFIDENCE_THRESHOLD = 0.3; // Filter tháº¥p Ä‘á»ƒ detect nhiá»u, nhÆ°ng >0.3 Ä‘á»ƒ giáº£m lá»—i
const IOU_THRESHOLD = 0.45; // NMS threshold

// COCO classes cho YOLOv8 (80 classes)
const CLASS_NAMES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
    "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
    "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
    "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
    "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear",
    "hair drier", "toothbrush"
];

// Sharpen kernel
const sharpenKernel = [0, -1, 0, -1, 5, -1, 0, -1, 0];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920 },
                height: { ideal: 1080 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale"
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera Ä‘Ã£ báº­t - SiÃªu nÃ©t";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message;
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-yolo.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
    updateStatus();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
    updateStatus();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
    updateStatus();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => CLASS_NAMES[p.class] === "person").length;
    status.innerText = `ğŸ” PhÃ¡t hiá»‡n ${predictions.length} váº­t thá»ƒ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i YOLOv8 siÃªu tá»‘i tÃ¢n...";
    try {
        model = await tf.loadGraphModel("./yolov8n_web_model/model.json"); // Chá»‰nh náº¿u dÃ¹ng yolov8s
        status.innerText = "âœ… YOLOv8 sáºµn sÃ ng - Detect siÃªu chÃ­nh xÃ¡c!";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i táº£i YOLOv8: " + err.message + " (kiá»ƒm tra folder model)";
    }
}

// Preprocess input cho YOLOv8
function preprocess(src) {
    let [input, xRatio, yRatio] = tf.tidy(() => {
        const img = tf.browser.fromPixels(src);
        const [h, w] = img.shape.slice(0, 2);
        const maxSize = Math.max(h, w);
        const imgPadded = img.pad([[0, maxSize - h], [0, maxSize - w], [0, 0]]);
        return [
            tf.expandDims(tf.div(tf.image.resizeBilinear(imgPadded, [INPUT_SIZE, INPUT_SIZE]), 255.0), 0),
            w / maxSize,
            h / maxSize
        ];
    });
    return [input, xRatio, yRatio];
}

// Postprocess output YOLOv8
function postprocess(output, xRatio, yRatio) {
    return tf.tidy(() => {
        const boxes = [];
        output = output.transpose([0, 2, 1]); // [1, 8400, 84] -> [1, 84, 8400] nhÆ°ng adjust cho YOLOv8
        output = output.squeeze(); // [84, 8400]
        const [classProbs, boxesData] = tf.split(output, [80, 4], 0); // Classes vÃ  boxes
        const scores = tf.max(classProbs, 0).dataSync();
        const classes = tf.argMax(classProbs, 0).dataSync();

        const rawBoxes = boxesData.transpose().dataSync();

        for (let i = 0; i < scores.length; ++i) {
            if (scores[i] > CONFIDENCE_THRESHOLD) {
                const [yCenter, xCenter, height, width] = rawBoxes.slice(i * 4, (i + 1) * 4);
                const top = (yCenter - height / 2) / yRatio;
                const left = (xCenter - width / 2) / xRatio;
                const bottom = (yCenter + height / 2) / yRatio;
                const right = (xCenter + width / 2) / xRatio;
                boxes.push({
                    top: Math.max(0, top),
                    left: Math.max(0, left),
                    bottom: Math.min(video.videoHeight, bottom),
                    right: Math.min(video.videoWidth, right),
                    class: classes[i],
                    score: scores[i]
                });
            }
        }

        // NMS Ä‘á»ƒ giáº£m trÃ¹ng láº·p
        const selected = tf.image.nonMaxSuppression(
            boxes.map(item => [item.top, item.left, item.bottom, item.right]),
            boxes.map(item => item.score),
            20, // Max output
            IOU_THRESHOLD,
            CONFIDENCE_THRESHOLD
        ).dataSync();

        return selected.map(index => boxes[index]);
    });
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;
            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }
            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    applySharpen(); // Sharpen má»—i frame

    frameCount++;
    if (frameCount % detectInterval === 0) {
        tf.tidy(() => {
            const [input, xRatio, yRatio] = preprocess(video);
            const res = model.execute(input);
            const predictions = postprocess(res, xRatio, yRatio);

            predictions.forEach(p => {
                const x = p.left;
                const y = p.top;
                const w = p.right - p.left;
                const h = p.bottom - p.top;
                const className = CLASS_NAMES[p.class];
                const isPerson = className === "person";

                ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.lineWidth = isPerson ? 4 : 2;
                ctx.strokeRect(x, y, w, h);

                ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
                ctx.font = isPerson ? "18px Arial" : "14px Arial";
                ctx.fillText(
                    `${className} (${Math.round(p.score * 100)}%)`,
                    x,
                    y > 25 ? y - 8 : y + 18
                );
            });

            updateStatus(predictions);
        });
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>    background: rgba(0,0,0,0.7);
    color: #0f0;
    border: 1px solid #0f0;
    border-radius: 30px;
    padding: 10px 16px;
    font-size: 16px;
    cursor: pointer;
}

#status {
    position: fixed;
    top: 10px;
    left: 10px;
    color: #0f0;
    background: rgba(0,0,0,0.6);
    padding: 8px 12px;
    border-radius: 8px;
    font-size: 14px;
    z-index: 10;
}
</style>
</head>

<body>

<div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
</div>

<div id="status">ğŸ“· Äang khá»Ÿi Ä‘á»™ng camera...</div>

<div class="controls">
    <button onclick="switchCamera()">ğŸ” Äá»•i camera</button>
    <button onclick="takePhoto()">ğŸ“¸ Chá»¥p áº£nh</button>
    <button onclick="zoomIn()">ğŸ” +</button>
    <button onclick="zoomOut()">ğŸ” -</button>
</div>

<div class="zoom-controls">
    <label>Zoom: <span id="zoomValue">1.0x</span></label>
    <input type="range" id="zoomSlider" class="zoom-slider" min="0.5" max="10" step="0.1" value="1" oninput="updateZoom(this.value)">
</div>

<script>
let video = document.getElementById("video");
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let status = document.getElementById("status");
let zoomValueDisplay = document.getElementById("zoomValue");
let zoomSlider = document.getElementById("zoomSlider");

let currentFacing = "environment";
let stream = null;
let model = null;

let zoomLevel = 1;
const MIN_ZOOM = 0.5;
const MAX_ZOOM = 10;
const ZOOM_STEP = 0.5;

let frameCount = 0;
const detectInterval = 3;

// Sharpen kernel (nháº¹, khÃ´ng quÃ¡ máº¡nh Ä‘á»ƒ trÃ¡nh noise)
const sharpenKernel = [
    0, -1, 0,
    -1, 5, -1,
    0, -1, 0
];

async function startCamera() {
    if (stream) stream.getTracks().forEach(t => t.stop());

    try {
        // Constraints cho rÃµ nÃ©t cao: Æ°u tiÃªn high-res, rear camera
        const constraints = {
            video: {
                facingMode: { ideal: currentFacing },
                width: { ideal: 1920, min: 1280 },
                height: { ideal: 1080, min: 720 },
                frameRate: { ideal: 30 },
                resizeMode: "crop-and-scale" // Giá»¯ cháº¥t lÆ°á»£ng cao khi scale
            },
            audio: false
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        status.innerText = "ğŸ“¹ Camera Ä‘Ã£ báº­t - Cháº¥t lÆ°á»£ng cao";
    } catch (err) {
        status.innerText = "âŒ Lá»—i camera: " + err.message + " (thá»­ giáº£m res náº¿u lá»—i)";
        // Fallback low res náº¿u high fail
        const fallback = { video: { facingMode: currentFacing } };
        stream = await navigator.mediaDevices.getUserMedia(fallback);
        video.srcObject = stream;
    }
}

function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    startCamera();
}

function takePhoto() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    // Ãp sharpen khi chá»¥p Ä‘á»ƒ áº£nh nÃ©t hÆ¡n
    applySharpen();
    const a = document.createElement("a");
    a.download = "ai-photo-net.png";
    a.href = canvas.toDataURL("image/png");
    a.click();
}

function applyZoom() {
    const scale = `scale(${zoomLevel})`;
    video.style.transform = `translate(-50%, -50%) ${scale}`;
    canvas.style.transform = `translate(-50%, -50%) ${scale}`;
    zoomValueDisplay.innerText = zoomLevel.toFixed(1) + 'x';
    zoomSlider.value = zoomLevel;
}

function zoomIn() {
    zoomLevel = Math.min(zoomLevel + ZOOM_STEP, MAX_ZOOM);
    applyZoom();
}

function zoomOut() {
    zoomLevel = Math.max(zoomLevel - ZOOM_STEP, MIN_ZOOM);
    applyZoom();
}

function updateZoom(value) {
    zoomLevel = parseFloat(value);
    applyZoom();
}

function updateStatus(predictions = []) {
    const personCount = predictions.filter(p => p.class === "person" && p.score > 0.5).length;
    status.innerText = `ğŸ” PhÃ¡t hiá»‡n ${predictions.length} váº­t thá»ƒ (person: ${personCount}) | Zoom: ${zoomLevel.toFixed(1)}x | Res: ${video.videoWidth}x${video.videoHeight}`;
}

function applySharpen() {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    const tempData = new Uint8ClampedArray(data);

    for (let y = 1; y < canvas.height - 1; y++) {
        for (let x = 1; x < canvas.width - 1; x++) {
            const i = (y * canvas.width + x) * 4;
            let r = 0, g = 0, b = 0;

            for (let ky = -1; ky <= 1; ky++) {
                for (let kx = -1; kx <= 1; kx++) {
                    const idx = ((y + ky) * canvas.width + (x + kx)) * 4;
                    const weight = sharpenKernel[(ky + 1) * 3 + (kx + 1)];
                    r += tempData[idx] * weight;
                    g += tempData[idx + 1] * weight;
                    b += tempData[idx + 2] * weight;
                }
            }

            data[i] = Math.min(255, Math.max(0, r));
            data[i + 1] = Math.min(255, Math.max(0, g));
            data[i + 2] = Math.min(255, Math.max(0, b));
            data[i + 3] = 255;
        }
    }
    ctx.putImageData(imageData, 0, 0);
}

async function loadAI() {
    status.innerText = "ğŸ¤– Äang táº£i AI...";
    try {
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        status.innerText = "âœ… AI sáºµn sÃ ng";
        detect();
    } catch (err) {
        status.innerText = "âŒ Lá»—i AI: " + err.message;
    }
}

async function detect() {
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        requestAnimationFrame(detect);
        return;
    }

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Ãp sharpen nháº¹ lÃªn frame Ä‘á»ƒ rÃµ nÃ©t hÆ¡n (má»—i frame)
    applySharpen();

    frameCount++;
    if (frameCount % detectInterval === 0) {
        const predictions = await model.detect(video);
        const filteredPreds = predictions.filter(p => p.score > 0.5);

        // Váº½ bounding box sau sharpen
        filteredPreds.forEach(p => {
            const [x, y, w, h] = p.bbox;
            const isPerson = p.class === "person";

            ctx.strokeStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.lineWidth = isPerson ? 4 : 2;
            ctx.strokeRect(x, y, w, h);

            ctx.fillStyle = isPerson ? "#ff3366" : "#00ff00";
            ctx.font = isPerson ? "18px Arial" : "14px Arial";
            ctx.fillText(
                `${p.class} (${Math.round(p.score * 100)}%)`,
                x,
                y > 25 ? y - 8 : y + 18
            );
        });

        updateStatus(filteredPreds);
    } else {
        updateStatus();
    }

    requestAnimationFrame(detect);
}

(async () => {
    await startCamera();
    await loadAI();
    applyZoom();
})();
</script>

</body>
</html>
